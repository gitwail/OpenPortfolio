{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "db0e92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "104bb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range((100)):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e4adc459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TileCoder:\n",
    "    def __init__(self, num_tilings, num_tiles, bounds, action_space):\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "        self.bounds = bounds\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.position_tile_size = (bounds[0][1] - bounds[0][0]) / num_tiles\n",
    "        self.velocity_tile_size = (bounds[1][1] - bounds[1][0]) / num_tiles\n",
    "\n",
    "        self.position_tiling_offset = np.linspace(0, self.position_tile_size, num_tilings, endpoint=False)\n",
    "        self.velocity_tiling_offset = np.linspace(0, self.velocity_tile_size, num_tilings, endpoint=False)\n",
    "\n",
    "    def encode(self, state, action):\n",
    "        position, velocity = state\n",
    "        position_indices = []\n",
    "        velocity_indices = []\n",
    "\n",
    "        for i in range(self.num_tilings):\n",
    "            position_tile_index = int((position - self.bounds[0][0] + self.position_tiling_offset[i]) / self.position_tile_size)\n",
    "            velocity_tile_index = int((velocity - self.bounds[1][0] + self.velocity_tiling_offset[i]) / self.velocity_tile_size)\n",
    "\n",
    "            position_indices.append(position_tile_index)\n",
    "            velocity_indices.append(velocity_tile_index)\n",
    "\n",
    "        # Include the action index\n",
    "        action_index = action\n",
    "        return tuple(position_indices + velocity_indices)\n",
    "\n",
    "# Example usage:\n",
    "tile_coder = TileCoder(num_tilings=8, num_tiles=8, bounds=[(-1.2, 0.6), (-0.07, 0.07)], action_space=3)\n",
    "state = [-0.50991637, 0.00546296]\n",
    "action = 2\n",
    "encoded_state = tile_coder.encode(state, action)\n",
    "print(encoded_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4ba3ccce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original state: [-0.5, 0.02]\n",
      "Discretized state: [19 31]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MountainCarDiscretizer:\n",
    "    def __init__(self, num_bins=20):\n",
    "        self.num_bins = num_bins\n",
    "        self.position_bins = np.linspace(-1.2, 0.6, num=num_bins - 1)\n",
    "        self.velocity_bins = np.linspace(-0.07, 0.07, num=num_bins - 1)\n",
    "\n",
    "    def discretize(self, state):\n",
    "        position, velocity = state\n",
    "        position_bin = np.digitize(position, self.position_bins)\n",
    "        velocity_bin = np.digitize(velocity, self.velocity_bins)\n",
    "        return np.array([position_bin, velocity_bin])\n",
    "\n",
    "# Example usage:\n",
    "discretizer = MountainCarDiscretizer(num_bins=50)\n",
    "\n",
    "# Example state\n",
    "state = [-0.5, 0.02]\n",
    "\n",
    "# Discretize the state\n",
    "discretized_state = discretizer.discretize(state)\n",
    "\n",
    "print(\"Original state:\", state)\n",
    "print(\"Discretized state:\", discretized_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "75e6c4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6, 6, 6, 6, 6, 6, 7, 8, 8, 8, 8, 8, 8, 8, 8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_coder.encode([0.2, 0.07], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c988b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(s,a,W):\n",
    "    \n",
    "    return W[a].T@s \n",
    "\n",
    "def pi(s,W,epsilon=0.2):\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: choose a random action\n",
    "        return np.random.choice(3)\n",
    "    else:\n",
    "        return np.argmax([Q(s,a,W) for a in range(3)])\n",
    " \n",
    "def feature(observation):\n",
    "    \n",
    "    return np.array(tile_coder.encode(observation, 2))\n",
    "\n",
    "def feature_discret(observation):\n",
    "    return discretizer.discretize(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3a9a4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "observation, info = env.reset(seed=42)\n",
    "s=observation\n",
    "a=action\n",
    "W=np.zeros((3,2))\n",
    "alpha=0.001\n",
    "eps=0.2\n",
    "gamma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3aad4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "s=scaler.transform((s.reshape(1, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "50493204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(5000):\n",
    "    terminated=False\n",
    "    while not (terminated):\n",
    "#         s=scaler.transform((s.reshape(1, 2)))\n",
    "        x=feature_discret(s.reshape(2,))\n",
    "#         x=feature(s)\n",
    "        a = pi(x,W,epsilon=0.2)  # this is where you would insert your policy\n",
    "        sp, R, terminated, truncated, info = env.step(a) \n",
    "#         x=x.reshape(3,1)\n",
    "        if terminated or truncated:\n",
    "            W[a]+=(alpha*(R-Q(x,a,W))*x).astype(float)\n",
    "            break\n",
    "        #goto next episode\n",
    "#         sp=scaler.transform((sp.reshape(1, 2)))\n",
    "        xp=feature_discret(sp.reshape(2,))\n",
    "#         xp=feature(sp)\n",
    "        ap=pi(xp,W,epsilon=0.2)\n",
    "        W[a]+=alpha*(R+(gamma*Q(xp,ap,W))-Q(x,a,W))*x\n",
    "       \n",
    "        \n",
    "        s=sp\n",
    "        a=ap\n",
    "   \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c1bfce6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01226655, -0.04932258],\n",
       "       [ 0.01251863, -0.04951416],\n",
       "       [ 0.01083569, -0.04823512]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3e1643e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(500):\n",
    "   x=feature_discret(observation) \n",
    "   action =  pi(x,W,epsilon=0)  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   if terminated :\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

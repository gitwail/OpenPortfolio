{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8f5aa5-6810-4497-8061-7c40a7ed0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib/myenv\")\n",
    "from gridworld import gridworld\n",
    "import pygame\n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# sample from categorical distribution\n",
    "def sample_categorical(probabilities):\n",
    "    return random.choices(range(len(probabilities)), probabilities)[0]\n",
    "\n",
    "# gridworld dimension\n",
    "dim=7\n",
    "\n",
    "# gamma discounting factor \n",
    "gamma=1\n",
    "\n",
    "#env variable \n",
    "gw=gridworld(dim)\n",
    "\n",
    "\n",
    "# The agent is placed at (0,0) and value function are initiliazed to a zero array\n",
    "gw.reset()\n",
    "\n",
    "\n",
    "# State value (V) is an array of dimension nxn where n is the gridworld size\n",
    "V=np.random.rand(dim,dim)\n",
    "# V=np.zeros((dim,dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38481b44-2beb-466f-9e0e-49cb55568d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ policy  ########################################\n",
    "\n",
    "def policy(dim):\n",
    "    \"\"\"\n",
    "    Initial policy with 1/4 chance for each action\n",
    "    Input: dimension of gridworld\n",
    "    output: random uniform policy\n",
    "    \"\"\"\n",
    "    pi={}\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            pi[(i,j)]=[0.25]*4\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834448af-1f54-4b90-9005-5448713da9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [0.25, 0.25, 0.25, 0.25],\n",
       " (0, 1): [0.25, 0.25, 0.25, 0.25],\n",
       " (0, 2): [0.25, 0.25, 0.25, 0.25],\n",
       " (0, 3): [0.25, 0.25, 0.25, 0.25],\n",
       " (1, 0): [0.25, 0.25, 0.25, 0.25]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial policy\n",
    "\n",
    "# The action mapping for human readibility\n",
    "# 0:right\n",
    "# 1:Left\n",
    "# 2:UP\n",
    "# 3:Down \n",
    "\n",
    "pi=policy(dim)\n",
    "sliced_policy = dict(list(pi.items())[:5])\n",
    "sliced_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb44aab-58f7-412f-bea2-5c586a8d3262",
   "metadata": {},
   "source": [
    "### Policy evaluation using montecarlo first visit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5a27d-5d3c-48ce-867c-e9d7f55f5a84",
   "metadata": {},
   "source": [
    "#### Estimating state value V(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f57f599-a337-4366-8821-34f71157641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def episode(pi,dim,log=True):\n",
    "    # reset environement\n",
    "    gw.reset()\n",
    "    # Init observation\n",
    "    o=(0,0,dim-1,dim-1)\n",
    "    # track state termination\n",
    "    terminated=False\n",
    "    # list of rewards\n",
    "    rewards=[]\n",
    "    # State/action/reward list\n",
    "    sar=[]\n",
    "    #Initial reward\n",
    "    r=-1\n",
    "    # episode length\n",
    "    l=0\n",
    "    while True :\n",
    "        l+=1\n",
    "        # sample an action \n",
    "        a=sample_categorical(pi[o[:2]])\n",
    "        # store the action/state/reward in a list\n",
    "        sar.append((o[:2],a,r))\n",
    "        if terminated:\n",
    "            break\n",
    "        o,r,terminated,_,_,=gw.step(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "    if log:\n",
    "        print(\"total rewards: \", np.sum(rewards))\n",
    "        # print(\"state action reward: \",sar)\n",
    "        print(\"length of episode: \",l)\n",
    "    return sar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24879949-fa10-4c1e-b891-5c7b61cc6a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards:  -15\n",
      "length of episode:  17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((0, 0), 2, -1),\n",
       " ((0, 0), 0, -1),\n",
       " ((1, 0), 3, -1),\n",
       " ((1, 1), 1, -1),\n",
       " ((0, 1), 3, -1),\n",
       " ((0, 2), 1, -1),\n",
       " ((0, 2), 1, -1),\n",
       " ((0, 2), 1, -1),\n",
       " ((0, 2), 1, -1),\n",
       " ((0, 2), 1, -1),\n",
       " ((0, 2), 0, -1),\n",
       " ((1, 2), 0, -1),\n",
       " ((2, 2), 3, -1),\n",
       " ((2, 3), 3, -1),\n",
       " ((2, 3), 3, -1),\n",
       " ((2, 3), 0, -1),\n",
       " ((3, 3), 1, 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode(pi,dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff6cc0b-bf77-424d-aa56-638597804b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): [], (0, 1): [], (0, 2): [], (0, 3): [], (1, 0): [], (1, 1): [], (1, 2): [], (1, 3): [], (2, 0): [], (2, 1): [], (2, 2): [], (2, 3): [], (3, 0): [], (3, 1): [], (3, 2): [], (3, 3): []}\n"
     ]
    }
   ],
   "source": [
    "#Initialization of the first visit MC algo\n",
    "\n",
    "# Gain initialization\n",
    "G=0\n",
    "# Gain for each state state dictionnary\n",
    "dict_visit={(i,j):[] for \n",
    "            i in range(dim) for j in range(dim)}\n",
    "print(dict_visit)\n",
    "# number of episodes for evaluation The state value V \n",
    "num_episodes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a76a84b4-28f6-47ad-a147-37c8be62cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards:  -198\n",
      "length of episode:  200\n",
      "total rewards:  -118\n",
      "length of episode:  120\n",
      "total rewards:  -14\n",
      "length of episode:  16\n",
      "total rewards:  -23\n",
      "length of episode:  25\n",
      "total rewards:  -13\n",
      "length of episode:  15\n",
      "total rewards:  -74\n",
      "length of episode:  76\n",
      "total rewards:  -17\n",
      "length of episode:  19\n",
      "total rewards:  -100\n",
      "length of episode:  102\n",
      "total rewards:  -138\n",
      "length of episode:  140\n",
      "total rewards:  -70\n",
      "length of episode:  72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(num_episodes):\n",
    "    G=0\n",
    "    #episode \n",
    "    sar=episode(pi,dim)\n",
    "    # reverse the list sar list\n",
    "    reversed_sar=list(reversed(sar))\n",
    "\n",
    "    for i,e in enumerate(reversed_sar):\n",
    "        s,a,r=e\n",
    "        G=G+(gamma*r)\n",
    "        Exist=sum([sar[0]==s for sar in reversed_sar[i+1:]])\n",
    "        if Exist==0 :\n",
    "            dict_visit[s].append(G)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48fb5aee-d83b-4f4d-aa28-2d7309a09ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list visit:  {(0, 0): [-199, -119, -15, -24, -14, -75, -18, -101, -139, -71], (0, 1): [-195, -111, -14, -74, -100, -134, -36], (0, 2): [-186, -107, -6, -73, -6, -99, -129, -37], (0, 3): [-153, -103, -5, -71, -8, -98, -109, -40], (1, 0): [-198, -116, -23, -13, -64, -16, -38, -138, -70], (1, 1): [-191, -95, -13, -15, -63, -43, -133, -59], (1, 2): [-170, -105, -7, -21, -4, -94, -132, -30], (1, 3): [-172, -101, -4, -11, -99, -42], (2, 0): [-144, -93, -6, -21, -12, -58, -15, -80, -91, -69], (2, 1): [-181, -96, -12, -19, -11, -62, -14, -69, -76, -60], (2, 2): [-174, -99, -13, -8, -22, -13, -93, -64, -44], (2, 3): [-173, -75, -1, -12, -92, -22, -43], (3, 0): [-179, -92, -3, -54, -83, -90, -67], (3, 1): [-180, -97, -11, -10, -61, -88, -60, -63], (3, 2): [-175, -98, -1, -10, -7, -25, -2, -89, -22], (3, 3): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "###############################################\n",
      "The states values estimated using MC First visit:\n",
      "  [[-77.5        -94.85714286 -80.375      -73.375     ]\n",
      " [-75.11111111 -76.5        -70.375      -71.5       ]\n",
      " [-58.9        -60.         -58.88888889 -59.71428571]\n",
      " [-81.14285714 -71.25       -47.66666667   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"list visit: \",dict_visit)\n",
    "for k,v in dict_visit.items():\n",
    "    if len(v)!=0:\n",
    "        V[k]=np.mean(v)\n",
    "    else:\n",
    "        V[k]=-1000\n",
    "\n",
    "print(\"###############################################\")\n",
    "print(\"The states values estimated using MC First visit:\\n \",V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd6032-7cc5-4996-8ea9-6e013b25c54f",
   "metadata": {},
   "source": [
    "#### Estimating action-state value Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212317b8-9d0d-4842-b59a-072ce50cc273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0, 0): [0.9467647492179863], (0, 0, 1): [0.31305442740265554], (0, 0, 2): [0.5122530156092381], (0, 0, 3): [0.7078797091789509], (0, 1, 0): [0.36798147798051384], (0, 1, 1): [0.47500734674889766], (0, 1, 2): [0.5606086657218193], (0, 1, 3): [0.37558926566651296], (0, 2, 0): [0.22534315262714], (0, 2, 1): [0.21943377055275537], (0, 2, 2): [0.985091289941047], (0, 2, 3): [0.8200759302121923], (0, 3, 0): [0.15775926125918005], (0, 3, 1): [0.4182238460976664], (0, 3, 2): [0.8669716970371708], (0, 3, 3): [0.1865117096049368], (1, 0, 0): [0.7389011238353007], (1, 0, 1): [0.5030967108087995], (1, 0, 2): [0.576458436352256], (1, 0, 3): [0.7305642534280589], (1, 1, 0): [0.3072170035346643], (1, 1, 1): [0.6831853435776136], (1, 1, 2): [0.3563411136076732], (1, 1, 3): [0.2506198542877752], (1, 2, 0): [0.7715460020324978], (1, 2, 1): [0.8643137605955304], (1, 2, 2): [0.04225263305805538], (1, 2, 3): [0.2103842387661461], (1, 3, 0): [0.3139358827853582], (1, 3, 1): [0.7210350166748491], (1, 3, 2): [0.8808652384525677], (1, 3, 3): [0.8599137902817939], (2, 0, 0): [0.1073320541114654], (2, 0, 1): [0.9758114671547481], (2, 0, 2): [0.62354767651185], (2, 0, 3): [0.32960575460183406], (2, 1, 0): [0.588999045755283], (2, 1, 1): [0.7171786391823972], (2, 1, 2): [0.33115364063191266], (2, 1, 3): [0.22856020969455826], (2, 2, 0): [0.2945477515430037], (2, 2, 1): [0.7678996628902607], (2, 2, 2): [0.9325020635494482], (2, 2, 3): [0.45132807309628487], (2, 3, 0): [0.6953731668867271], (2, 3, 1): [0.25504674431989516], (2, 3, 2): [0.26623557661648267], (2, 3, 3): [0.8616683066297798], (3, 0, 0): [0.0724296363706518], (3, 0, 1): [0.16971589807451504], (3, 0, 2): [0.7769541840474564], (3, 0, 3): [0.1693015254235971], (3, 1, 0): [0.8432543605018084], (3, 1, 1): [0.6443361759290823], (3, 1, 2): [0.05131697140787528], (3, 1, 3): [0.1122565183822728], (3, 2, 0): [0.002253461393123657], (3, 2, 1): [0.1551094263001075], (3, 2, 2): [0.7300097191286014], (3, 2, 3): [0.6094240794193946], (3, 3, 0): [0.4614737347643757], (3, 3, 1): [0.2520280999499228], (3, 3, 2): [0.5990780262182397], (3, 3, 3): [0.4171469954349083]}\n"
     ]
    }
   ],
   "source": [
    "#Initialization of the first visit MC algo\n",
    "\n",
    "# Gain initialization\n",
    "G=0\n",
    "# Gain for each state state dictionnary\n",
    "dict_visit_action={(i,j,a):[np.random.rand()] for \n",
    "            i in range(dim) for j in range(dim) for a in range(4)}\n",
    "print(dict_visit_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab8df823-8c6e-4b9b-8f3c-483c5e8e7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for evaluation The state value V \n",
    "\n",
    "def Policy_Evaluation(pi,dim=4,num_episodes=100):\n",
    "    Q={}\n",
    "    for _ in range(num_episodes):\n",
    "        G=0\n",
    "        #episode \n",
    "        sar=episode(pi,dim,log=False)\n",
    "        # reverse the list sar list\n",
    "        reversed_sar=list(reversed(sar))\n",
    "\n",
    "        for i,e in enumerate(reversed_sar):\n",
    "            s,a,r=e\n",
    "            G=G+(gamma*r)\n",
    "            Exist_state_action=sum([(sar[0]==s and sar[1]==a) for sar in reversed_sar[i+1:]])\n",
    "            if Exist_state_action==0 :\n",
    "                dict_visit_action[s+(a,)].append(G)\n",
    "\n",
    "    for k,v in dict_visit_action.items():\n",
    "            Q[k]=np.mean(v)\n",
    "      \n",
    "    return Q\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c45e97-a74f-4102-a65d-cad28c822e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "The Action states values estimated using MC First visit:\n",
      "  {(0, 0, 0): -58.92566544063477, (0, 0, 1): -61.00941021332325, (0, 0, 2): -63.54459323857299, (0, 0, 3): -58.23817432457804, (0, 1, 0): -49.5643335905836, (0, 1, 1): -60.6949072713565, (0, 1, 2): -60.08917034974227, (0, 1, 3): -58.80036433931841, (0, 2, 0): -45.93797417014179, (0, 2, 1): -51.65490566445632, (0, 2, 2): -60.01879460574183, (0, 2, 3): -51.88588086411349, (0, 3, 0): -34.86898384550513, (0, 3, 1): -41.21292835897561, (0, 3, 2): -50.86535349613653, (0, 3, 3): -50.697661845686355, (1, 0, 0): -49.98880452842674, (1, 0, 1): -59.80828172148652, (1, 0, 2): -59.38256324220799, (1, 0, 3): -54.06013289783904, (1, 1, 0): -46.964504353791774, (1, 1, 1): -52.905280244273705, (1, 1, 2): -53.15809276862939, (1, 1, 3): -45.953562573510176, (1, 2, 0): -39.14186989651668, (1, 2, 1): -55.75339215598511, (1, 2, 2): -44.573291617906335, (1, 2, 3): -36.68128918076359, (1, 3, 0): -21.320885848407976, (1, 3, 1): -37.89270374962103, (1, 3, 2): -37.321683718330796, (1, 3, 3): -33.030002268676796, (2, 0, 0): -43.873083356176586, (2, 0, 1): -54.68674879476167, (2, 0, 2): -54.57911127190792, (2, 0, 3): -50.11500679733445, (2, 1, 0): -42.32178433243617, (2, 1, 1): -55.025153360016034, (2, 1, 2): -46.119086542131576, (2, 1, 3): -44.50460067441196, (2, 2, 0): -27.129669369283114, (2, 2, 1): -46.8846420067422, (2, 2, 2): -52.16827376039167, (2, 2, 3): -25.07393066514383, (2, 3, 0): -0.967396669867563, (2, 3, 1): -28.95816510852267, (2, 3, 2): -35.8814934562051, (2, 3, 3): -32.84553326773481, (3, 0, 0): -42.81865565034947, (3, 0, 1): -58.79575710254814, (3, 0, 2): -49.64312260048341, (3, 0, 3): -35.85063955155368, (3, 1, 0): -44.188335411565745, (3, 1, 1): -44.0762650869107, (3, 1, 2): -51.36996237224549, (3, 1, 3): -24.383812351854946, (3, 2, 0): -31.92299025148488, (3, 2, 1): -37.142403354581475, (3, 2, 2): -37.653870654221656, (3, 2, 3): -0.9678115184116121, (3, 3, 0): 0.015912887405668128, (3, 3, 1): 0.013264636839469622, (3, 3, 2): 0.02218807504511999, (3, 3, 3): 0.014384379152927873}\n"
     ]
    }
   ],
   "source": [
    "Q=Policy_Evaluation(pi,dim=4,num_episodes=100)\n",
    "print(\"###############################################\")\n",
    "print(\"The Action states values estimated using MC First visit:\\n \",Q)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09cd5e65-2518-4f11-9dfd-bc52753d13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_human={0:\"right\",1:\"left\",2:\"UP\",3:\"DOWN\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a51a8934-8fe8-44de-aead-4ad1eaa251ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy improvement \n",
    "def Policy_Improvement(pi,Q,log=False):\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            s=(i,j)\n",
    "            a=np.argmax([Q[s+(a,)] for a in range(4)])\n",
    "            if log:\n",
    "                print(f\"In state {s} the action to take is {action_human[a]}\")\n",
    "            pi[s]=[1 if i == a else 0 for i in range(4)]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "618babdb-c8a8-4c8c-bae7-4c06f01e17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0, 0): -8.2099487199169, (0, 0, 1): -60.76731259726021, (0, 0, 2): -62.90609683730488, (0, 0, 3): -58.44518021358759, (0, 1, 0): -49.70990498240499, (0, 1, 1): -59.69440287542166, (0, 1, 2): -60.091991304775405, (0, 1, 3): -56.62956598456505, (0, 2, 0): -44.402729013240204, (0, 2, 1): -47.89088712683241, (0, 2, 2): -57.697195586516045, (0, 2, 3): -47.40276806261212, (0, 3, 0): -34.12440437706802, (0, 3, 1): -38.47373659544264, (0, 3, 2): -48.330203567233795, (0, 3, 3): -50.71659275093942, (1, 0, 0): -6.584444930888076, (1, 0, 1): -59.95134587901654, (1, 0, 2): -59.35592923116901, (1, 0, 3): -53.003246213814116, (1, 1, 0): -45.25911556574297, (1, 1, 1): -50.433097352234604, (1, 1, 2): -53.66171748453323, (1, 1, 3): -44.32228168850256, (1, 2, 0): -37.74612077139953, (1, 2, 1): -53.37011604570213, (1, 2, 2): -43.79298011693559, (1, 2, 3): -34.3837034800199, (1, 3, 0): -23.453869903560896, (1, 3, 1): -40.27099928537399, (1, 3, 2): -36.34844489926053, (1, 3, 3): -31.93349037697019, (2, 0, 0): -5.177314023148792, (2, 0, 1): -54.04957686119418, (2, 0, 2): -54.420283660749796, (2, 0, 3): -50.328556438339106, (2, 1, 0): -38.49858731673404, (2, 1, 1): -53.480679704139966, (2, 1, 2): -44.577146960587584, (2, 1, 3): -42.232936663754245, (2, 2, 0): -28.62895100477802, (2, 2, 1): -43.853868338951834, (2, 2, 2): -48.07532403586019, (2, 2, 3): -24.542477865448397, (2, 3, 0): -0.972206997264152, (2, 3, 1): -32.08318040168728, (2, 3, 2): -36.107585555076895, (2, 3, 3): -30.71922613190608, (3, 0, 0): -43.06808303171231, (3, 0, 1): -58.08149540642395, (3, 0, 2): -51.367306828220904, (3, 0, 3): -3.820953332650817, (3, 1, 0): -41.93387780556973, (3, 1, 1): -42.43251229303834, (3, 1, 2): -49.298717075714805, (3, 1, 3): -2.5084622506638072, (3, 2, 0): -30.374929579331464, (3, 2, 1): -37.93661442863824, (3, 2, 2): -37.886969402450646, (3, 2, 3): -0.9992818277200272, (3, 3, 0): 0.0007528119653578722, (3, 3, 1): 0.0007263057635444461, (3, 3, 2): 0.0007573679218941084, (3, 3, 3): 0.000754334530623704}\n"
     ]
    }
   ],
   "source": [
    "# policy iteration for montecarlo \n",
    "\n",
    "Q=Policy_Evaluation(pi,dim=4,num_episodes=20)\n",
    "print(Q)\n",
    "pi=Policy_Improvement(pi,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "79330c0b-61aa-4a9a-8bc0-d1597a3db621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [1, 0, 0, 0],\n",
       " (0, 1): [1, 0, 0, 0],\n",
       " (0, 2): [1, 0, 0, 0],\n",
       " (0, 3): [1, 0, 0, 0],\n",
       " (1, 0): [1, 0, 0, 0],\n",
       " (1, 1): [0, 0, 0, 1],\n",
       " (1, 2): [0, 0, 0, 1],\n",
       " (1, 3): [1, 0, 0, 0],\n",
       " (2, 0): [1, 0, 0, 0],\n",
       " (2, 1): [1, 0, 0, 0],\n",
       " (2, 2): [0, 0, 0, 1],\n",
       " (2, 3): [1, 0, 0, 0],\n",
       " (3, 0): [0, 0, 0, 1],\n",
       " (3, 1): [0, 0, 0, 1],\n",
       " (3, 2): [0, 0, 0, 1],\n",
       " (3, 3): [1, 0, 0, 0]}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The action mapping for human readibility\n",
    "# 0:right\n",
    "# 1:Left\n",
    "# 2:UP\n",
    "# 3:Down \n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "80447dbf-e4c8-4d10-a817-e4afa0ac0b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  right\n",
      "state:  (1, 0)\n",
      "reward is:  -1\n",
      "Action:  right\n",
      "state:  (2, 0)\n",
      "reward is:  -1\n",
      "Action:  right\n",
      "state:  (3, 0)\n",
      "reward is:  -1\n",
      "Action:  DOWN\n",
      "state:  (3, 1)\n",
      "reward is:  -1\n",
      "Action:  DOWN\n",
      "state:  (3, 2)\n",
      "reward is:  -1\n",
      "Action:  DOWN\n",
      "state:  (3, 3)\n",
      "reward is:  0\n",
      "total rewards:  -5\n"
     ]
    }
   ],
   "source": [
    "# The loop below will test the policy iteration algorithm\n",
    "# start position\n",
    "V=np.zeros((dim,dim))\n",
    "gw.reset()\n",
    "o=(0,0,dim-1,dim-1)\n",
    "terminated=False\n",
    "rewards=[]\n",
    "stop=0\n",
    "while (not terminated) and (stop!=20) :\n",
    "    stop=stop+1\n",
    "    a=sample_categorical(pi[o[:2]])\n",
    "    o,r,terminated,_,_,=gw.step(a)\n",
    "    rewards.append(r)\n",
    "    gw.render(V,mode='human')\n",
    "    print(\"Action: \",action_human[a])\n",
    "    print(\"state: \",o[:2])\n",
    "    print(\"reward is: \",r)\n",
    "\n",
    "print(\"total rewards: \", np.sum(rewards))\n",
    "\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
